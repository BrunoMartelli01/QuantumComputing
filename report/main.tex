\documentclass{ceurart}

\usepackage{lineno}
\linenumbers
\usepackage[inline]{enumitem}
\usepackage{physics}
\usepackage{todonotes}
\usepackage{graphicx}
\usepackage{float}
\usepackage{caption}
\usepackage{subcaption}

\begin{document}

\copyrightyear{2025}
\copyrightclause{Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).}
\conference{BigHPC2025: Special Track on Big Data and High-Performance Computing, co-located with the 4\textsuperscript{th} Italian Conference on Big Data and Data Science, ITADATA2025, September 09 -- 11, 2025, Turin, Italy.}
\title{End2End Quantum Learning}

\author[1]{Mario Bifulco}[email=mario.bifulco@unito.it,url=https://github.com/TheFlonet/End2EndQuantumSVM]
\cormark[1]
\author[1]{Luca Roversi}[orcid=0000-0002-1871-6109,email=luca.roversi@unito.it,url=https://www.di.unito.it/~rover/]
\address[1]{Università degli studi di Torino, Dipartimento di Informatica, Corso Svizzera 185 - 10149 Torino}
\cortext[1]{Corresponding author.}

\begin{abstract}
  TODO
\end{abstract}

\begin{keywords}
  Quantum Machine Learning \sep
  Quantum Support Vector Machine \sep 
  Quantum High Performance Computing \sep
  Quantum Gate \sep
  Quantum Annealing
\end{keywords}

\maketitle

\section{Introduction}

Quantum computing ultimately aims to expand the boundaries of what is currently considered computable.
This goal can be pursued by focusing on two main directions:
\begin{enumerate*}
  \item Performing certain classes of computations more efficiently than classical computers;
  \item Enhancing the expressive capacity of currently adopted computational methods;
\end{enumerate*}
Among the various fields where such advancements appear promising is machine learning.

When discussing quantum computing, it is essential to specify the computational paradigm being considered.
\emph{Gate-based quantum computing} refers to a universal quantum computing model, typically implemented using superconducting circuits, where quantum circuits are programmed analogously to how logic gates define classical circuits.
\emph{Annealing-based quantum computing}, on the other hand, refers to a non-universal subset of adiabatic quantum computing, primarily designed for solving optimization problems.

In the context of quantum machine learning, support vector machines (SVMs) have been studied under both computational paradigms\cite{qsvm_qa, qsvm_gqc}.
Within SVMs, we can distinguish two principal components that contribute to model training:
\begin{enumerate*}
  \item The \emph{kernel method}, which enables the mapping of examples into a higher-dimensional feature space, thereby increasing the likelihood of linear separability, as per Cover's theorem\cite{th_cover};
  \item The \emph{optimization problem}, which identifies the model parameters used for inference on new data instances.
\end{enumerate*}

In the literature, the term \emph{Quantum Support Vector Machine (QSVM)} is used with two distinct meanings.
In the quantum annealing framework, QSVM refers to the use of classical kernels combined with a reformulation of the optimization problem to be solved via quantum annealing.
In the gate-based approach, by contrast, quantum computing is employed to compute the kernels, while the optimization is handled by classical processors.

This work aims to unify the aforementioned approaches to construct a fully quantum learning pipeline for support vector machines.
Moreover, the use of different types of QPUs enables experimentation within the domain of Quantum High Performance Computing (QHPC).
In this domain, traditional CPU and GPU systems can collaborate with various QPUs—regardless of their underlying architecture—with the goal of addressing computationally intensive problems.

\section{Quantum Kernel}

The use of quantum kernels enables the mapping of input examples into a hyperspace of dimension $O(2^{\#\text{features}})$.
This property stems from the fact that qubits can exist in a superposition of states, allowing the simultaneous representation of both $\ket{0}$ and $\ket{1}$ during computation.
However, this superposition is lost upon measurement, which—according to the postulates of quantum mechanics—causes the wave function to collapse into one of the two basis states.

Quantum kernel\cite{qkernel} methods are based on the construction of a quantum circuit that is executed for each pair of input data points.
Specifically, given two classical input vectors $\vec{x}_i$ and $\vec{x}_j$, the quantum kernel evaluates the similarity between them by computing the fidelity (i.e., squared inner product) of their corresponding quantum states in Hilbert space.

The typical procedure involves the following steps:
\begin{enumerate*}
  \item Each input $\vec{x}$ is encoded into a quantum state $\ket{\phi(\vec{x})}$ using a feature map circuit;
  \item For each pair $(\vec{x}_i, \vec{x}_j)$, a quantum circuit is constructed to prepare the state $\ket{\phi(\vec{x}_i)}$, followed by the inverse of the circuit that prepares $\ket{\phi(\vec{x}_j)}$;
  \item The overlap between the states is then estimated by measuring the probability of the resulting state collapsing to the all-zero state, i.e., computing $|\langle\phi(\vec{x}_i)|\phi(\vec{x}_j)\rangle|^2$.
\end{enumerate*}

This procedure allows the definition of a kernel matrix $K$, where each entry $K_{ij}$ corresponds to the quantum fidelity between examples $\vec{x}_i$ and $\vec{x}_j$.

\begin{figure}
  \centering
  \begin{minipage}[b]{0.49\textwidth}
    \centering
    \includegraphics[width=\linewidth]{imgs/hr_map.png}
    \subcaption{\texttt{SU2HR} feature map\label{fig:map_a}}
  \end{minipage}
  \hfill
  \begin{minipage}[b]{0.49\textwidth}
    \centering
    \includegraphics[width=\linewidth]{imgs/rr_map.png}
    \subcaption{\texttt{SU2RR} feature map\label{fig:map_b}}
  \end{minipage}
  \caption{Examples of feature maps on two qubits. In addition to those shown, the \texttt{z\_feature\_map} and \texttt{zz\_feature\_map} available in the \emph{Qiskit} library were used.}
\end{figure}

In our experiments, we evaluated various quantum kernels by tuning the following hyperparameters:
\begin{description}
  \item[Number of qubits available:] 
  Determines the upper limit of encodable features. When the number of qubits was insufficient, we applied Principal Component Analysis (PCA) to project the data into an \emph{n}-dimensional subspace capturing the most relevant variance.

  \item[Feature map:] 
  The quantum circuit responsible for encoding classical data into quantum states. These circuits determine the geometry of the induced Hilbert space and affect model expressivity.
  Figure~\ref{fig:map_a} and Figure~\ref{fig:map_b} shows a couple of possible feature maps.

  \item[Repetitions of the feature map:] 
  Defines how many times the feature map circuit is applied in sequence. Increasing repetitions enhances the circuit's expressiveness but also deepens the circuit, which may lead to increased noise on real hardware.
\end{description}

To evaluate the quality of the kernel derived from the various feature maps, we employed the \textit{Kernel-Target Alignment} (KTA) strategy\cite{kta}.
This metric allows for estimating the effectiveness of a given kernel on a reference dataset without the need to train a machine learning model.

Formally, given a kernel matrix $K \in \mathbb{R}^{n \times n}$ computed over a set of $n$ examples, and a label vector $\mathbf{y} \in \{-1, 1\}^n$, the KTA is defined as:

$$\text{KTA}(K, \mathbf{y}) = \frac{\mathbf{y}^\top K \mathbf{y}}{\|K\|_F \cdot n}$$

where $\|K\|_F$ denotes the Frobenius norm of the kernel matrix.
This normalized inner product quantifies the alignment between the kernel matrix and the ideal target kernel implied by the labels.
A higher value of KTA indicates a stronger alignment and, consequently, a potentially more suitable kernel for classification tasks.

\section{Quantum Annealing Support Vector Machine}

Annealing-based quantum computing naturally addresses quadratic optimization problems, particularly those formulated as QUBO (Quadratic Unconstrained Binary Optimization) instances.
The dual form of the support vector machine (SVM) problem\cite{svm} leads to a quadratic objective, which can be adapted to QUBO by incorporating constraints into the objective function and converting variables into binary form.

Constraint incorporation is typically achieved through Lagrangian relaxation\cite{lagrangian_relaxation}. 
However, the binarization process is less straightforward due to the continuous nature of the optimization variables, which lie in $\mathbb{R}$.

Integer variables can be automatically encoded in binary format, provided that an upper bound is defined for each variable. 
This additional hyperparameter is crucial, as omitting it would result in each variable being represented with 64 bits by default, leading to problems that exceed the capacity of current quantum annealers.
While the binarization of real-valued variables incurs significantly more overhead.

Empirically, we found that treating all optimization variables as integers yields a degradation in model performance of less than one percentage point.
This trade-off led us to favor an approximated yet more hardware-efficient formulation during model generation.

An additional hyperparameter of practical importance is the regularization parameter $C$, which controls the trade-off between maximizing the margin and minimizing classification errors.
Higher values of $C$ emphasize correct classification of training examples, whereas lower values allow for a softer margin with more tolerance to misclassified points.

In our experiments, we explored a range of values for $C$ by selecting powers of two, from $2^1$ to $2^{12}$.
The choice of using powers of two is motivated by the fact that the binarization process creates, for each optimization variable, $\log_2(C)$ binary variables.
By setting $C$ to a power of two, we ensure that the number of binary variables per original variable is minimized and evenly distributed, thereby optimizing the use of the limited number of qubits available on current quantum annealing hardware.

\section{A Complete QSVM}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Top 5 kernel params:
% Qubit, Feature map, Map repetitions, Alignment
% 1. 30, SU2HR, 1, 98.650\%
% 2. 8, ZMAP, 3, 93.981\%
% 3. 8, ZMAP, 2, 93.558\%
% 4. 16, SU2RR, 1, 91.203\%
% 5. 8, SU2RR, 1, 89.596\%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Top 3 C params:
% 1. 7
% 2. 63
% 3. 255
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Conclusion}

\bibliography{refs.bib}

\end{document}
