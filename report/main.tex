\documentclass{ceurart}

\usepackage{lineno}
\linenumbers
\usepackage[inline]{enumitem}
\usepackage{physics}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{booktabs}

\usepackage{ulem} % \sout
\newcommand{\LH}[1]{\textcolor{blue}{{ #1}}}
\newcommand{\LHs}[1]{\textcolor{blue}{\sout {#1}}}
\newcommand{\LM}[1]{\textcolor{red}{{ #1}}}
\newcommand{\LMs}[1]{\textcolor{red}{\sout {#1}}}
\begin{document}

\copyrightyear{2025}
\copyrightclause{Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).}
\conference{BigHPC2025: Special Track on Big Data and High-Performance Computing, co-located with the 4\textsuperscript{th} Italian Conference on Big Data and Data Science, ITADATA2025, September 09 -- 11, 2025, Turin, Italy.}
\title{End2End Quantum Learning}

\author[1]{Mario Bifulco}[email=mario.bifulco@unito.it,url=https://github.com/TheFlonet/End2EndQuantumSVM]
\cormark[1]
\author[1]{Luca Roversi}[orcid=0000-0002-1871-6109,email=luca.roversi@unito.it,url=https://www.di.unito.it/~rover/]
\address[1]{Università degli studi di Torino, Dipartimento di Informatica, Corso Svizzera 185 - 10149 Torino}
\cortext[1]{Corresponding author.}

\begin{abstract}
    This work presents a fully quantum approach to support vector machine (SVM) learning by integrating gate-based quantum kernel methods with quantum annealing-based optimization. 
    We explore the construction of quantum kernels using various feature maps and qubit configurations, evaluating their suitability through Kernel-Target Alignment (KTA). 
    The SVM dual problem is reformulated as a Quadratic Unconstrained Binary Optimization (QUBO) problem, enabling its solution via quantum annealers. 
    Our experiments demonstrate that a high degree of alignment in the kernel and an appropriate regularization parameter lead to competitive performance, with the best model achieving an F1-score of 90\%. 
    These results highlight the feasibility of an end-to-end quantum learning pipeline and the potential of hybrid quantum architectures in quantum high-performance computing (QHPC) contexts.
\end{abstract}

\begin{keywords}
  Quantum Machine Learning \sep
  Quantum Support Vector Machine \sep
  Quantum High Performance Computing \sep
  Quantum Gate \sep
  Quantum Annealing
\end{keywords}

\maketitle

%---------------------
\section{Introduction}

Quantum computing ultimately aims to expand the boundaries of what is currently considered efficiently computable.
Among the various fields where such advancements appear promising is machine learning.

When discussing quantum computing, it is essential to specify the computational paradigm being considered.
\emph{Gate-based quantum computing} refers to a universal quantum computing model, typically implemented using superconducting circuits, where quantum circuits are programmed analogously to how logic gates define classical circuits.
\emph{Annealing-based quantum computing}, on the other hand, refers to a non-universal subset of adiabatic quantum computing, primarily designed for solving optimization problems.

In the context of quantum machine learning, support vector machines (SVMs) have been studied under both computational paradigms\cite{qsvm_qa, qsvm_gqc}.
Within SVMs, we can distinguish two principal components that contribute to model training:
\begin{enumerate*}
  \item The \emph{kernel method}, which enables the mapping of examples into a higher-dimensional feature space, thereby increasing the likelihood of linear separability, as per Cover's theorem\cite{th_cover};
  \item The \emph{optimization problem}, which identifies the model parameters used for inference on new data instances.
\end{enumerate*}

In the literature, the term \emph{Quantum Support Vector Machine (QSVM)} is used with two distinct meanings.
In the quantum annealing framework, QSVM refers to the use of classical kernels combined with a reformulation of the optimization problem to be solved via quantum annealing.
In the gate-based approach, by contrast, quantum computing is employed to compute the kernels, while the optimization is handled by classical processors.

This work aims to unify the aforementioned approaches to construct a fully quantum learning pipeline for support vector machines \LM{using a kernel based on quantum gates and a model optimisation process based on quantum annealing}.
Moreover, the use of different types of QPUs enables experimentation within the domain of Quantum High Performance Computing (QHPC).
In this domain, traditional CPU and GPU systems can collaborate with various QPUs—regardless of their underlying architecture—with the goal of addressing computationally intensive problems.

%---------------------
\section{A short recap about gate-based generated Quantum Kernels}

\LM{In this section, we recall the theoretical foundations underlying the construction of a gate-based quantum circuit employed to implement a quantum kernel.}

The use of quantum kernels enables the mapping of input examples into a hyperspace of dimension $O(2^{\#\text{features}})$.
This property stems from the fact that states formed by qubits can be superposed during a computation.

Quantum kernel\cite{qkernel} methods are based on the construction of a quantum circuit that is executed for each pair of input data points.

Specifically, given two classical input vectors $\vec{x}_i$ and $\vec{x}_j$, the quantum kernel evaluates the similarity between them by computing the fidelity (i.e., squared inner product) of their corresponding quantum states in Hilbert space.

The typical procedure involves the following steps:
\begin{enumerate*}
  \item Each input $\vec{x}$ is encoded into a quantum state $\ket{\phi(\vec{x})}$ using a feature map circuit;
  \item For each pair $(\vec{x}_i, \vec{x}_j)$, a quantum circuit is constructed to prepare the state $\ket{\phi(\vec{x}_i)}$, followed by the inverse of the circuit that prepares $\ket{\phi(\vec{x}_j)}$;
  \item The overlap between the states is then estimated by measuring the probability of the resulting state collapsing to the all-zero state, i.e., computing $|\langle\phi(\vec{x}_i)|\phi(\vec{x}_j)\rangle|^2$.
\end{enumerate*}

This procedure allows the definition of a kernel matrix $K$, where each entry $K_{ij}$ corresponds to the \LM{similarity} between examples $\vec{x}_i$ and $\vec{x}_j$ \LM{mapped in kernel space}.

%---------------------
\section{Quantum Annealing Support Vector Machine}

Annealing-based quantum computing naturally addresses quadratic optimization problems, particularly those formulated as QUBO (Quadratic Unconstrained Binary Optimization) instances.
The dual formulation of the support vector machine (SVM) problem\cite{svm} leads to a \LM{multivariate quadratic polynomial, which serves as the objective function to be optimized.
By incorporating constraints into the objective via penalty terms and converting continuous variables into their binary expansion, the problem can be reformulated as a QUBO model.}

Constraint incorporation is typically achieved through Lagrangian relaxation\cite{lagrangian_relaxation}.
However, the binarization process is less straightforward due to the continuous nature of the optimization variables, which lie in $\mathbb{R}$.
On one hand, integer variables can be automatically encoded in binary format, provided that an upper bound is defined for each variable.
This additional hyperparameter is crucial, as omitting it would result in each variable being represented with 64 bits by default, leading to problems that exceed the capacity of current quantum annealers.
On the other hand, the binarization of real-valued variables incurs significantly more overhead.

Empirically, we found that treating all optimization variables as integers yields a degradation in model performance of less than one percentage point.
This trade-off led us to favor an approximated yet more hardware-efficient formulation during model generation.

An additional hyperparameter of practical importance is the regularization parameter $C$, which controls the trade-off between maximizing the margin and minimizing classification errors.
Higher values of $C$ emphasize correct classification of training examples, whereas lower values allow for a softer margin with more tolerance to misclassified points.

%---------------------
\section{Results}

\LM{Both the design of a kernel and the optimization of the SVM dual problem require the selection of hyperparameters prior to training.
Section \ref{sec:qkernel} presents our investigation into the parametrization of the quantum kernel with respect to the chosen dataset.
Section \ref{sec:qasvm} reports on a set of experiments aimed at selecting the parameter $C$, assuming no kernel is employed during training.}

\LM{Once the optimal hyperparameters for the individual components of the problem have been identified, Section \ref{sec:qkernel_and_qasvm} presents the results of the end-to-end learning pipeline for fully quantum SVMs—namely, those employing a kernel generated via quantum gates and optimization performed through quantum annealing.}

\subsection{Finding a good Quantum Kernel}\label{sec:qkernel}

\begin{figure}
  \centering
  \begin{minipage}[b]{0.49\textwidth}
    \centering
    \includegraphics[width=\linewidth]{imgs/hr_map.png}
    \subcaption{\texttt{SU2HR} feature map\label{fig:map_a}}
  \end{minipage}
  \hfill
  \begin{minipage}[b]{0.49\textwidth}
    \centering
    \includegraphics[width=\linewidth]{imgs/rr_map.png}
    \subcaption{\texttt{SU2RR} feature map\label{fig:map_b}}
  \end{minipage}
  \caption{Examples of feature maps on two qubits. In addition to those shown, the \texttt{z\_feature\_map} and \texttt{zz\_feature\_map} available in the \emph{Qiskit} library were used.}
\end{figure}

To identify the optimal hyperparameters for the quantum kernel, we evaluated several values for the following parameters:
\begin{description}
  \item[Number of qubits available:]
  Determines the upper limit of encodable features. When the number of qubits was insufficient, we applied Principal Component Analysis (PCA) to project the data into an \emph{n}-dimensional subspace capturing the most relevant variance.

  \item[Feature map:]
  The quantum circuit responsible for encoding classical data into quantum states. These circuits determine the geometry of the induced Hilbert space and affect model expressivity.
  Figure~\ref{fig:map_a} and Figure~\ref{fig:map_b} shows a couple of possible feature maps.

  \item[Repetitions of the feature map:]
  Defines how many times the feature map circuit is applied in sequence. Increasing repetitions enhances the circuit's expressiveness but also deepens the circuit, which may lead to increased noise on real hardware.
\end{description}

To evaluate the quality of the kernel derived from the various feature maps, we employed the \textit{Kernel-Target Alignment} (KTA) strategy\cite{kta}.
This metric allows for estimating the effectiveness of a given kernel on a reference dataset without the need to train a machine learning model.

Formally, given a kernel matrix $K \in \mathbb{R}^{n \times n}$ computed over a set of $n$ examples, and a label vector $\mathbf{y} \in \{-1, 1\}^n$, the KTA is defined as:

$$\text{KTA}(K, \mathbf{y}) = \frac{\mathbf{y}^\top K \mathbf{y}}{\|K\|_F \cdot n}$$
where $\|K\|_F$ denotes the Frobenius norm of the kernel matrix.
This normalized inner product quantifies the alignment between the kernel matrix and the ideal target kernel implied by the labels.
A higher value of KTA indicates a stronger alignment and, consequently, a potentially more suitable kernel for classification tasks.

\LM{The top three configurations for the quantum kernel are reported in Table~\ref{tab:top_kernel}; all proposed configurations exhibit a high degree of alignment.
In this study, no single hyperparameter emerged as universally optimal across configurations. 
This is evidenced by the presence of both circuits with a large number of qubits and those with fewer qubits, deep circuits as well as shallow ones, and the use of three distinct feature maps.}

\begin{table}
  \centering
  \caption{Top threen kernel configurations.}
  \label{tab:top_kernel}
  \begin{tabular}{ccccr}
    \toprule
    \textbf{\# Qubits} & \textbf{Feature Map} & \textbf{Repetitions} & \textbf{Alignment (KTA)} \\
    \midrule
    30 & SU2HR & 1 & 98.650\% \\
    8  & ZMAP  & 2 & 93.558\% \\
    16 & SU2RR & 1 & 91.203\% \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{Selecting regularization parameter for Quantum Annealing SVM}\label{sec:qasvm}

In our experiments, we explored a range of values for $C$ by selecting powers of two, from $2^2 - 1$ to $2^{12} - 1$.
The choice of using powers of two minus one is motivated by the fact that the binarization process creates, for each optimization variable, $\log_2(C)$ binary variables.
By setting $C$ to a power of two minus one, we ensure that the number of binary variables per original variable is minimized and evenly distributed, thereby optimizing the use of the limited number of qubits available on current quantum annealing hardware.

We observed the best performance using regularization parameter values of $C = 7, 63, 255$.
\LM{Nevertheless, the differences in performance across the various configurations are minimal, suggesting that—in the application context we investigated—the examples are likely to be linearly separable with not excessive rate.}

\subsection{Fully Quantum SVM}\label{sec:qkernel_and_qasvm}

\LM{By combining quantum kernels with optimization via quantum annealing, we constructed several QSVM models, obtaining a wide range of results.}

\LM{The best-performing model was based on the kernel with the highest alignment (using 30 qubits) and a regularization parameter $C=255$. 
In this case, we achieved an \texttt{F1-score} of 90\%.
We also observed that the negative class exhibited higher \texttt{recall} but lower \texttt{precision}, whereas the opposite was true for the positive class.
This suggests that the trained model suffers from classification issues that lead to the generation of false negatives.
False negatives were found across all models, indicating that this is a systematic issue. 
A more effective preprocessing phase would likely result in a uniform performance improvement across all configurations.}

\LM{The model with the smallest number of qubits yielded the poorest results.
Using only 8 qubits to encode the input features and setting $C=63$, the resulting \texttt{F1-score} was 52\%, making this model comparable to random guessing.
This behavior may be attributed to an insufficient number of features used to represent the data: reducing from 30 to 8 qubits likely caused a significant loss of information, leading to examples becoming overly similar and thus harder to distinguish.}

%---------------------
\section{Conclusion}

\LM{In this study, we have explored the feasibility and effectiveness of constructing an end-to-end quantum learning pipeline for support vector machines. 
By integrating gate-based quantum kernel methods with quantum annealing-based optimization, we demonstrated that it is possible to train a fully quantum SVM model on real-world datasets.}

\LM{Our results show that the choice of quantum kernel—particularly the number of qubits and the selected feature map—significantly influences classification performance. 
Furthermore, we observed that the approximation involved in representing real-valued optimization variables with discrete binary variables in the QUBO formulation does not lead to substantial performance degradation.}

\LM{The best-performing configuration, employing 30 qubits and a regularization parameter $C=255$, achieved an F1-score of 90\%. 
However, the presence of systematic false negatives suggests that further improvements could be achieved with a more refined data preprocessing pipeline.}

\LM{This work opens the door for future investigations into scalable quantum machine learning pipelines and highlights the role of quantum high-performance computing in facilitating hybrid computational workflows across diverse quantum architectures.}
\bibliography{refs.bib}

\end{document}
